{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import dgl.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import  roc_auc_score\n",
    "from dgl.nn.pytorch.glob import SumPooling\n",
    "from dataLoader import \n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        num_layers,\n",
    "        hidden_channels=None,\n",
    "        bias=True,\n",
    "        dropout=0.0,\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.out_channels = out_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.dropout = dropout\n",
    "        self.fs = nn.ModuleList()\n",
    "\n",
    "        for _ in range(in_channels):\n",
    "            if num_layers == 1:\n",
    "                curr_f = [nn.Linear(1, out_channels, bias=bias)]\n",
    "            else:\n",
    "                curr_f = [\n",
    "                    nn.Linear(1, hidden_channels, bias=bias),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(p=dropout),\n",
    "                ]\n",
    "                for _ in range(1, num_layers - 1):\n",
    "                    curr_f.append(\n",
    "                        nn.Linear(hidden_channels, hidden_channels, bias=bias)\n",
    "                    )\n",
    "                    curr_f.append(nn.ReLU())\n",
    "                    curr_f.append(nn.Dropout(p=dropout))\n",
    "                curr_f.append(nn.Linear(hidden_channels, out_channels, bias=bias))\n",
    "            self.fs.append(nn.Sequential(*curr_f))\n",
    "\n",
    "    def init_params(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fx = torch.empty(x.size(0), x.size(1), self.out_channels).to(self.device)\n",
    "        for feature_index in range(x.size(1)):\n",
    "            feature_col = x[:, feature_index]\n",
    "            feature_col = feature_col.view(-1, 1)\n",
    "            feature_col = self.fs[feature_index](feature_col)\n",
    "            fx[:, feature_index] = feature_col\n",
    "\n",
    "        f_sums = fx.sum(dim=1)\n",
    "        return f_sums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorGNAN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        num_layers,\n",
    "        hidden_channels=None,\n",
    "        bias=True,\n",
    "        dropout=0.0,\n",
    "        device=\"cpu\",\n",
    "        limited_m=True,\n",
    "        normalize_m=True,\n",
    "        is_graph_task=False,\n",
    "        readout_n_layers=1,\n",
    "        final_agg=\"sum\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.out_channels = out_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.dropout = dropout\n",
    "        self.limited_m = limited_m\n",
    "        self.normalize_m = normalize_m\n",
    "        self.fs = nn.ModuleList()\n",
    "        self.is_graph_task = is_graph_task\n",
    "        self.readout_n_layers = readout_n_layers\n",
    "\n",
    "        self.actual_output_dim_f = (\n",
    "            1 if is_graph_task and readout_n_layers > 0 else out_channels\n",
    "        )\n",
    "        self.actual_output_dim_m = (\n",
    "            1 if limited_m or (is_graph_task and readout_n_layers > 0) else out_channels\n",
    "        )\n",
    "\n",
    "        for _ in range(in_channels):\n",
    "            if num_layers == 1:\n",
    "                curr_f = [nn.Linear(1, self.actual_output_dim_f, bias=bias)]\n",
    "            else:\n",
    "                curr_f = [\n",
    "                    nn.Linear(1, hidden_channels, bias=bias),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(p=dropout),\n",
    "                ]\n",
    "                for _ in range(1, num_layers - 1):\n",
    "                    curr_f.append(\n",
    "                        nn.Linear(hidden_channels, hidden_channels, bias=bias)\n",
    "                    )\n",
    "                    curr_f.append(nn.ReLU())\n",
    "                    curr_f.append(nn.Dropout(p=dropout))\n",
    "                curr_f.append(\n",
    "                    nn.Linear(hidden_channels, self.actual_output_dim_f, bias=bias)\n",
    "                )\n",
    "            self.fs.append(nn.Sequential(*curr_f))\n",
    "\n",
    "        m_bias = True\n",
    "        if is_graph_task:\n",
    "            m_bias = False\n",
    "        if num_layers == 1:\n",
    "            self.m = [nn.Linear(1, self.actual_output_dim_m, bias=m_bias)]\n",
    "\n",
    "        else:\n",
    "            self.m = [nn.Linear(1, hidden_channels, bias=m_bias), nn.ReLU()]\n",
    "            for _ in range(1, num_layers - 1):\n",
    "                self.m.append(nn.Linear(hidden_channels, hidden_channels, bias=m_bias))\n",
    "                self.m.append(nn.ReLU())\n",
    "            self.m.append(\n",
    "                nn.Linear(hidden_channels, self.actual_output_dim_m, bias=m_bias)\n",
    "            )\n",
    "        self.m = nn.Sequential(*self.m)\n",
    "\n",
    "        if is_graph_task and self.readout_n_layers > 0:\n",
    "            self.readout_nam = NAM(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                readout_n_layers,\n",
    "                hidden_channels,\n",
    "                bias,\n",
    "                dropout,\n",
    "                device,\n",
    "            )\n",
    "\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "\n",
    "    def forward(self, inputs, feats):\n",
    "        x, node_distances = (\n",
    "            feats,\n",
    "            inputs.ndata[\"distance_matrix\"],\n",
    "        )\n",
    "        fx = torch.empty(x.size(0), x.size(1), self.actual_output_dim_f).to(self.device) # (N, D, out)\n",
    "        for feature_index in range(x.size(1)):\n",
    "            feature_col = x[:, feature_index]\n",
    "            feature_col = feature_col.view(-1, 1)\n",
    "            feature_col = self.fs[feature_index](feature_col)\n",
    "            fx[:, feature_index] = feature_col\n",
    "\n",
    "        fx_perm = torch.permute(fx, (2, 0, 1))\n",
    "        if self.normalize_m:\n",
    "            node_distances = torch.div(node_distances, inputs.ndata[\"normalization_distance_matrix\"])\n",
    "        m_dist = self.m(node_distances.flatten().view(-1, 1)).view(\n",
    "            x.size(0), x.size(0), self.actual_output_dim_m\n",
    "        )\n",
    "        m_dist_perm = torch.permute(m_dist, (2, 0, 1))\n",
    "\n",
    "        mf = torch.matmul(m_dist_perm, fx_perm)\n",
    "\n",
    "        if not self.is_graph_task:\n",
    "            out = torch.sum(mf, dim=2)\n",
    "\n",
    "        else:\n",
    "            hidden = torch.sum(mf, dim=1)\n",
    "            if self.readout_n_layers > 0:\n",
    "                out = self.readout_nam(hidden)\n",
    "            else:\n",
    "                out = torch.sum(hidden, dim=1).view(1, -1)\n",
    "        return out.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNAN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim,\n",
    "        out_dim,  \n",
    "        bias=True,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.distance_transform = nn.ModuleList()\n",
    "        self.feature_transform = nn.ModuleList()\n",
    "\n",
    "        for _ in range(in_dim):\n",
    "            self.distance_transform.append(nn.Linear(1, out_dim))\n",
    "            self.feature_transform.append(nn.Linear(1, out_dim))\n",
    "\n",
    "    def forward(self, graph, feats):\n",
    "        \"\"\"\n",
    "        params:\n",
    "\n",
    "            dis_matrix: shape (N,N) where N is number of nodes each element coresponds to node pair distance\n",
    "            feats: shape (N, d) where d is the feature dim\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        distance_matrix = graph.ndata[\"distance_matrix\"]  # (N, N)\n",
    "        normalization_distance_matrix = graph.ndata[\n",
    "            \"normalization_distance_matrix\"\n",
    "        ]  # N, N\n",
    "        num_nodes, feat_dim = feats.shape\n",
    "        f_matrix = torch.empty(feat_dim, num_nodes, self.out_dim)  # F matrix\n",
    "        m_matrix = torch.empty(feat_dim, num_nodes, num_nodes, self.out_dim)  # M matrix\n",
    "        distance_matrix = torch.div(distance_matrix, normalization_distance_matrix)\n",
    "        distance_matrix = distance_matrix.unsqueeze(-1)  # (N,N,1)\n",
    "        for k in range(feat_dim):\n",
    "            # x_k is the kth feature of all nodes\n",
    "            x_k = feats[:, k].view(-1, 1)  # shape (N, 1)\n",
    "            f_matrix[k, :, :] = self.feature_transform[k](x_k)  # (N, out)\n",
    "            m_matrix[k, :, :, :] = self.distance_transform[k](\n",
    "                distance_matrix\n",
    "            )  # (N, N, out)\n",
    "        \n",
    "        f_matrix = f_matrix.permute(2, 0, 1).unsqueeze(-1)\n",
    "        m_matrix = m_matrix.permute(3, 0, 1, 2)\n",
    "        m_f_matrix = torch.matmul(m_matrix, f_matrix)  # (out, d, N, 1)\n",
    "        h = m_f_matrix.sum(1)  # (out, N, 1)\n",
    "        h = h.permute(2, 1, 0)  # (1,N,out)\n",
    "        h = h.squeeze(0)  # (N,out)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNANMODULE(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hiddem_dim=24, num_layers=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hiddem_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.pool = SumPooling()\n",
    "        self.gnan = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                self.gnan.append(GNAN(in_dim, hiddem_dim))\n",
    "            elif i == num_layers - 1:\n",
    "                self.gnan.append(GNAN(hiddem_dim, out_dim))\n",
    "            else:\n",
    "                self.gnan.append(GNAN(hiddem_dim, hiddem_dim))\n",
    "\n",
    "    def forward(self, graph, feats):\n",
    "        for i in range(self.num_layers):\n",
    "            if i == 0:\n",
    "                h = F.relu(self.gnan[i](graph, feats))\n",
    "                h = F.dropout(h,0.2)\n",
    "            elif i == self.num_layers - 1:\n",
    "                h = self.gnan[i](graph, h)\n",
    "            else:\n",
    "                h = F.relu(self.gnan[i](graph, h))\n",
    "                h = F.dropout(h,0.2)\n",
    "\n",
    "        h = self.pool(graph, h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a373k/Documents/Github/interpretable-GNNs/GNAN/dataLoader.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(f\"{processed_data_dir}/{data_name}.pt\")\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader, test_loader, num_feats, num_class = getData()\n",
    "# num_class = data.num_classes\n",
    "# Define model, loss function, and optimizer\n",
    "model = GNANMODULE(num_feats, 1, hiddem_dim=28)\n",
    "# model = TensorGNAN(num_feats,1,2,28, is_graph_task=True)\n",
    "loss_fn = nn.BCEWithLogitsLoss()  # Binary Cross Entropy with Logits\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "# Define training settings\n",
    "num_epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_accuracy(outputs, labels):\n",
    "    if outputs.dim() == 2 and outputs.shape[-1] > 1:\n",
    "        return get_multiclass_accuracy(outputs, labels)\n",
    "    else:\n",
    "        y_prob = torch.sigmoid(outputs).view(-1)\n",
    "        y_prob = y_prob > 0.5\n",
    "        return (labels == y_prob).sum().item()\n",
    "\n",
    "\n",
    "def get_multiclass_accuracy(outputs, labels):\n",
    "    assert outputs.size(1) >= labels.max().item() + 1\n",
    "    probas = torch.softmax(outputs, dim=-1)\n",
    "    preds = torch.argmax(probas, dim=-1)\n",
    "    correct = (preds == labels).sum()\n",
    "    acc = correct\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model,\n",
    "    dloader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    classify=True,\n",
    "    label_index=0,\n",
    "    compute_auc=False,\n",
    "    is_graph_task=True,\n",
    "    epoch=-1\n",
    "):\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        running_loss = 0.0\n",
    "        n_samples = 0\n",
    "        all_probas = np.array([])\n",
    "        all_labels = np.array([])\n",
    "        if classify:\n",
    "            running_acc = 0.0\n",
    "        for graph, label in dloader:\n",
    "            if len(label.shape) > 1:\n",
    "                labels = label[:, label_index].view(-1, 1).flatten()\n",
    "                labels = labels.float()\n",
    "            else:\n",
    "                labels = label.flatten()\n",
    "            if -1 in labels:\n",
    "                labels = (labels + 1) / 2\n",
    "            # if loss_fn.__class__.__name__ == \"CrossEntropyLoss\":\n",
    "            #     labels = labels.long()\n",
    "\n",
    "            # non_zero_ids = None\n",
    "            # if model.__class__.__name__ == \"GNAM\":\n",
    "            #     labels = labels[data.train_mask]\n",
    "            #     non_zero_ids = torch.nonzero(data.train_mask).flatten()\n",
    "            # data = data.to(device)\n",
    "            # labels = labels.to(device)\n",
    "            # optimizer.zero_grad()\n",
    "            # if non_zero_ids is not None:\n",
    "            #     outputs = model.forward(data, non_zero_ids)\n",
    "            # else:\n",
    "            outputs = model.forward(graph, graph.ndata[\"feat\"])\n",
    "            # Check for NaN in the outputs\n",
    "            if torch.isnan(outputs).any():\n",
    "                print(f\"NaN detected in model output at epoch{epoch}\")\n",
    "                break\n",
    "            # if not is_graph_task:\n",
    "            #         labels = labels[data.train_mask]\n",
    "            #         outputs = outputs[data.train_mask]\n",
    "\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "\n",
    "            n_samples += len(labels)\n",
    "            if outputs.dim() == 2 and outputs.shape[-1] == 1:\n",
    "                loss = loss_fn(outputs.flatten(), labels.float())\n",
    "            else:\n",
    "                loss = loss_fn(outputs, labels)\n",
    "            if torch.isnan(loss).any():\n",
    "                print(f\"NaN detected in loss at epoch {epoch}\")\n",
    "                break\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if compute_auc:\n",
    "                probas = torch.sigmoid(outputs).view(-1)\n",
    "                all_probas = np.concatenate((all_probas, probas.detach().cpu().numpy()))\n",
    "                all_labels = np.concatenate((all_labels, labels.detach().cpu().numpy()))\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if classify:\n",
    "                running_acc += get_accuracy(outputs, labels)\n",
    "\n",
    "        if compute_auc:\n",
    "            auc = roc_auc_score(all_labels, all_probas)\n",
    "\n",
    "        if classify:\n",
    "            if compute_auc:\n",
    "                return running_loss / len(dloader), running_acc / n_samples, auc\n",
    "            else:\n",
    "                return running_loss / len(dloader), running_acc / n_samples, -1\n",
    "        else:\n",
    "            return running_loss / len(dloader), -1\n",
    "\n",
    "\n",
    "def test_epoch(\n",
    "    model,\n",
    "    dloader,\n",
    "    loss_fn,\n",
    "    classify=True,\n",
    "    label_index=0,\n",
    "    compute_auc=False,\n",
    "    val_mask=False,\n",
    "    is_graph_task=True,\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.0\n",
    "        all_probas = np.array([])\n",
    "        all_labels = np.array([])\n",
    "        n_samples = 0\n",
    "        if classify:\n",
    "            running_acc = 0.0\n",
    "        model.eval()\n",
    "        for graph, label in dloader:\n",
    "            if len(label.shape) > 1:\n",
    "                labels = label[:, label_index].view(-1, 1).flatten()\n",
    "                labels = labels.float()\n",
    "            else:\n",
    "                labels = label.flatten()\n",
    "            if -1 in labels:\n",
    "                labels = (labels + 1) / 2\n",
    "            # if loss_fn.__class__.__name__ == \"CrossEntropyLoss\":\n",
    "            #     labels = labels.long()\n",
    "\n",
    "            # non_zero_ids = None\n",
    "            # if model.__class__.__name__ == \"GNAM\":\n",
    "            #     if val_mask:\n",
    "            #         labels = labels[data.val_mask]\n",
    "            #         non_zero_ids = torch.nonzero(data.val_mask).flatten()\n",
    "            #     else:\n",
    "            #         labels = labels[data.test_mask]\n",
    "            #         non_zero_ids = torch.nonzero(data.test_mask).flatten()\n",
    "\n",
    "            # forward\n",
    "            # if non_zero_ids is not None:\n",
    "            #     outputs = model.forward(inputs, non_zero_ids)\n",
    "            # else:\n",
    "            outputs = model.forward(graph, graph.ndata[\"feat\"])\n",
    "            # if not is_graph_task:\n",
    "            #     if val_mask:\n",
    "            #         outputs = outputs[data.val_mask]\n",
    "            #         labels = labels[data.val_mask]\n",
    "            # else:\n",
    "\n",
    "            n_samples += len(labels)\n",
    "            if outputs.dim() == 2 and outputs.shape[-1] == 1:\n",
    "                loss = loss_fn(outputs.flatten(), labels.float())\n",
    "            else:\n",
    "                loss = loss_fn(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if classify:\n",
    "                running_acc += get_accuracy(outputs, labels)\n",
    "            if compute_auc:\n",
    "                probas = torch.sigmoid(outputs).view(-1)\n",
    "                all_probas = np.concatenate((all_probas, probas.detach().cpu().numpy()))\n",
    "                all_labels = np.concatenate((all_labels, labels.detach().cpu().numpy()))\n",
    "\n",
    "        if compute_auc:\n",
    "            auc = roc_auc_score(all_labels, all_probas)\n",
    "        if classify:\n",
    "            if compute_auc:\n",
    "                return running_loss / len(dloader), running_acc / n_samples, auc\n",
    "            else:\n",
    "                return running_loss / len(dloader), running_acc / n_samples, -1\n",
    "        else:\n",
    "            return running_loss / len(dloader), -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    loss, tran_acc, _ = train_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        classify=True,\n",
    "        compute_auc=False,\n",
    "        epoch=epoch + 1,\n",
    "    )\n",
    "\n",
    "    valid_loss, accuracy, auc = test_epoch(model, valid_loader, loss_fn)\n",
    "    print(\n",
    "        f\"loss : {loss:.4f} |loss, tran_acc : {tran_acc:.4f} |valid_loss : {loss:.4f} | valid_accuracy : {accuracy:4f} | valid_auc : {auc:4f}\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DGLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
