{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import dgl.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import  roc_auc_score\n",
    "from dgl.nn.pytorch.glob import SumPooling\n",
    "from dataLoader import getData\n",
    "import numpy as np\n",
    "# Set print options to print the full tensor\n",
    "# torch.set_printoptions(threshold=float(\"inf\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNAN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim,\n",
    "        hidden_dim,\n",
    "        num_layers,\n",
    "        out_dim,\n",
    "        feat_trasform_bias=True,\n",
    "        dist_transform_bias=False,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.distance_transform = nn.ModuleList()\n",
    "        self.feature_transform = nn.ModuleList()\n",
    "\n",
    "        for _ in range(in_dim):\n",
    "            distance_transform_layers = []\n",
    "            feature_transform_layers = []\n",
    "            if num_layers == 1:\n",
    "                distance_transform_layers.append(\n",
    "                    nn.Linear(1, out_dim, bias=dist_transform_bias)\n",
    "                )\n",
    "                feature_transform_layers.append(\n",
    "                    nn.Linear(1, out_dim, bias=feat_trasform_bias)\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                distance_transform_layers.append(\n",
    "                    nn.Linear(1, hidden_dim, bias=dist_transform_bias)\n",
    "                )\n",
    "                distance_transform_layers.append(nn.ReLU())\n",
    "\n",
    "                feature_transform_layers.append(\n",
    "                    nn.Linear(1, hidden_dim, bias=feat_trasform_bias)\n",
    "                )\n",
    "                feature_transform_layers.append(nn.ReLU())\n",
    "                feature_transform_layers.append(nn.Dropout(p=dropout))\n",
    "\n",
    "                for _ in range(1, num_layers - 1):\n",
    "                    distance_transform_layers.append(\n",
    "                        nn.Linear(hidden_dim, hidden_dim, bias=dist_transform_bias)\n",
    "                    )\n",
    "                    distance_transform_layers.append(nn.ReLU())\n",
    "\n",
    "                    feature_transform_layers.append(\n",
    "                        nn.Linear(hidden_dim, hidden_dim, bias=feat_trasform_bias)\n",
    "                    )\n",
    "                    feature_transform_layers.append(nn.ReLU())\n",
    "                    feature_transform_layers.append(nn.Dropout(p=dropout))\n",
    "\n",
    "                distance_transform_layers.append(\n",
    "                    nn.Linear(hidden_dim, out_dim, bias=dist_transform_bias)\n",
    "                )\n",
    "                feature_transform_layers.append(\n",
    "                    nn.Linear(hidden_dim, out_dim, bias=feat_trasform_bias)\n",
    "                )\n",
    "                self.distance_transform.append(\n",
    "                    nn.Sequential(*distance_transform_layers)\n",
    "                )\n",
    "                self.feature_transform.append(nn.Sequential(*feature_transform_layers))\n",
    "\n",
    "    def forward(self, graph, feats):\n",
    "        \"\"\"\n",
    "        params:\n",
    "\n",
    "            dis_matrix: shape (N,N) where N is number of nodes each element coresponds to node pair distance\n",
    "            feats: shape (N, d) where d is the feature dim\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        distance_matrix = graph.ndata[\"distance_matrix\"]  # (N, N)\n",
    "\n",
    "        normalization_distance_matrix = graph.ndata[\n",
    "            \"normalization_distance_matrix\"\n",
    "        ]  # N, N\n",
    "\n",
    "        num_nodes, feat_dim = feats.shape\n",
    "        f_matrix = torch.empty(\n",
    "            feat_dim, num_nodes, num_nodes, self.out_dim\n",
    "        )  # F matrix # (d, N, N, C)\n",
    "        m_matrix = torch.empty(\n",
    "            feat_dim, num_nodes, num_nodes, self.out_dim\n",
    "        )  # M matrix # (d, N, N, C)\n",
    "        # print(f\"noramlization_distance_matrix is {normalization_distance_matrix}\")\n",
    "        distance_matrix = torch.div(distance_matrix, normalization_distance_matrix)\n",
    "        # print(f\"distance_matrix is {distance_matrix}\")\n",
    "        distance_matrix = distance_matrix.view(-1, 1)  # (N*N,1)\n",
    "        # print(f\"distance_matrix.view(-1, 1) is {distance_matrix}\")\n",
    "        for k in range(feat_dim):\n",
    "            # x_k is the kth feature of all nodes\n",
    "            # Target sizes: [16, 16, 1].  Tensor sizes: [16, 28]\n",
    "            x_k = feats[:, k].view(-1, 1)  # shape (N, 1)\n",
    "            # print(f\"x_k shape is {x_k.shape}\")\n",
    "            # print(\n",
    "            #     f\"self.feature_transform[k](x_k) shape is {self.feature_transform[k](x_k).shape}\"\n",
    "            # )\n",
    "            # print(f\"f_matrix[k, :, :, :] shape is {f_matrix[k, :, :, :].shape}\")\n",
    "            # print(f\"self.feature_trasform is {self.feature_transform}\")\n",
    "            f_matrix[k, :, :, :] = self.feature_transform[k](x_k).repeat(\n",
    "                num_nodes, 1, 1\n",
    "            )  # (N, N, out)\n",
    "            m_matrix[k, :, :, :] = self.distance_transform[k](\n",
    "                distance_matrix\n",
    "            ).view(  # (N, N, out)\n",
    "                num_nodes, num_nodes, -1\n",
    "            )\n",
    "            # print(f\"m_matrix[k, :, :, :]shape {m_matrix[k, :, :, :].shape}\")\n",
    "            # m_matrix[k, :, :, :] = torch.div(\n",
    "            #     m_matrix[k, :, :, :], normalization_distance_matrix.unsqueeze(-1)\n",
    "            # )\n",
    "        # torch.set_printoptions(threshold=float(\"inf\"))\n",
    "        # print(f\"m matrix  before division is {m_matrix}\")\n",
    "        # m_matrix = torch.div(\n",
    "        #     m_matrix, normalization_distance_matrix.unsqueeze(-1).unsqueeze(0)\n",
    "        # )\n",
    "        # print(f\"m_matrix after divsion is {m_matrix}\")\n",
    "        # f matrix shape (d, N, N, out)\n",
    "        # m matrix shape (d, N, N, out)\n",
    "        f_matrix = f_matrix.permute(3, 0, 1, 2)  # (out, d, N, N)\n",
    "        m_matrix = m_matrix.permute(3, 0, 1, 2)  # (out, d, N, N)\n",
    "        m_f_matrix = f_matrix * m_matrix  # (out, d, N, N)\n",
    "        # print(f\"m_f_matrix is {m_f_matrix}\")\n",
    "        # print(f\"m_f_matrix is {m_f_matrix.shape}\")\n",
    "        # print(f\"m_f_matrix is {m_f_matrix}\")\n",
    "        h = m_f_matrix.sum(3)  # (out, d, N)\n",
    "        # print(f\"h 1ist is is of shape {h.shape}\")\n",
    "        # print(f\"h = m_f_matrix.shape is {h.shape}\")\n",
    "        h = h.sum(1)  # (out, N)\n",
    "        # print(f\"h = h.sum(1)is {h.shape}\")\n",
    "        # print(f\"h 2nd is shape is {h.shape}\")\n",
    "        # h = h.permute(1, 0)  # (N, out)\n",
    "        # print(f\"h.permute(1, 0) is {h}\")\n",
    "        # print(h)\n",
    "        h = h.sum(1) # (out,\n",
    "        # print(f\"output is {h.T}\")\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader, num_feats, num_class = getData()\n",
    "# num_class = data.num_classes\n",
    "# Define model, loss function, and optimizer\n",
    "model = GNAN(in_dim=num_feats, out_dim=1, hidden_dim=64, num_layers=3)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()  # Binary Cross Entropy with Logits\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-5,\n",
    ")\n",
    "\n",
    "# Define training settings\n",
    "num_epochs = 1000\n",
    "# train_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(outputs, labels):\n",
    "    if outputs.dim() == 2 and outputs.shape[-1] > 1:\n",
    "        return get_multiclass_accuracy(outputs, labels)\n",
    "    else:\n",
    "        y_prob = torch.sigmoid(outputs).view(-1)\n",
    "        y_prob = y_prob > 0.5\n",
    "        return (labels == y_prob).sum().item()\n",
    "\n",
    "\n",
    "def get_multiclass_accuracy(outputs, labels):\n",
    "    assert outputs.size(1) >= labels.max().item() + 1\n",
    "    probas = torch.softmax(outputs, dim=-1)\n",
    "    preds = torch.argmax(probas, dim=-1)\n",
    "    correct = (preds == labels).sum()\n",
    "    acc = correct\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model,\n",
    "    dloader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    classify=True,\n",
    "    label_index=0,\n",
    "    compute_auc=False,\n",
    "    is_graph_task=True,\n",
    "    epoch=-1,\n",
    "):\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        running_loss = 0.0\n",
    "        n_samples = 0\n",
    "        all_probas = np.array([])\n",
    "        all_labels = np.array([])\n",
    "        if classify:\n",
    "            running_acc = 0.0\n",
    "\n",
    "        for graph, label in dloader:\n",
    "            if len(label.shape) > 1:\n",
    "                labels = label[:, label_index].view(-1, 1).flatten()\n",
    "                labels = labels.float()\n",
    "            else:\n",
    "                labels = label.flatten()\n",
    "            if -1 in labels:\n",
    "                labels = (labels + 1) / 2\n",
    "            # if loss_fn.__class__.__name__ == \"CrossEntropyLoss\":\n",
    "            #     labels = labels.long()\n",
    "\n",
    "            # non_zero_ids = None\n",
    "            # if model.__class__.__name__ == \"GNAM\":\n",
    "            #     labels = labels[data.train_mask]\n",
    "            #     non_zero_ids = torch.nonzero(data.train_mask).flatten()\n",
    "            # data = data.to(device)\n",
    "            # labels = labels.to(device)\n",
    "            # optimizer.zero_grad()\n",
    "            # if non_zero_ids is not None:\n",
    "            #     outputs = model.forward(data, non_zero_ids)\n",
    "            # else:\n",
    "            outputs = model.forward(graph, graph.ndata[\"feat\"])\n",
    "            # print(\"train model output\")\n",
    "            # print(outputs)\n",
    "\n",
    "            # Check for NaN in the outputs\n",
    "            # if torch.isnan(outputs).any():\n",
    "            #     print(f\"NaN detected in model output at epoch{epoch}\")\n",
    "            #     break\n",
    "\n",
    "            # if not is_graph_task:\n",
    "            #         labels = labels[data.train_mask]\n",
    "            #         outputs = outputs[data.train_mask]\n",
    "\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "\n",
    "            n_samples += len(labels)\n",
    "            if outputs.dim() == 2 and outputs.shape[-1] == 1:\n",
    "                loss = loss_fn(outputs.flatten(), labels.float())\n",
    "            else:\n",
    "                loss = loss_fn(outputs, labels.float())\n",
    "            # if torch.isnan(loss).any():\n",
    "            #     print(f\"NaN detected in loss at epoch {epoch}\")\n",
    "            #     break\n",
    "\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            if compute_auc:\n",
    "                probas = torch.sigmoid(outputs).view(-1)\n",
    "                all_probas = np.concatenate((all_probas, probas.detach().cpu().numpy()))\n",
    "                all_labels = np.concatenate((all_labels, labels.detach().cpu().numpy()))\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if classify:\n",
    "                running_acc += get_accuracy(outputs, labels)\n",
    "\n",
    "        if compute_auc:\n",
    "            auc = roc_auc_score(all_labels, all_probas)\n",
    "\n",
    "        if classify:\n",
    "            if compute_auc:\n",
    "                return running_loss / len(dloader), running_acc / n_samples, auc\n",
    "            else:\n",
    "                return running_loss / len(dloader), running_acc / n_samples, -1\n",
    "        else:\n",
    "            return running_loss / len(dloader), -1\n",
    "\n",
    "\n",
    "def test_epoch(\n",
    "    model,\n",
    "    dloader,\n",
    "    loss_fn,\n",
    "    classify=True,\n",
    "    label_index=0,\n",
    "    compute_auc=False,\n",
    "    val_mask=False,\n",
    "    is_graph_task=True,\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.0\n",
    "        all_probas = np.array([])\n",
    "        all_labels = np.array([])\n",
    "        n_samples = 0\n",
    "        if classify:\n",
    "            running_acc = 0.0\n",
    "        model.eval()\n",
    "        for graph, label in dloader:\n",
    "            if len(label.shape) > 1:\n",
    "                labels = label[:, label_index].view(-1, 1).flatten()\n",
    "                labels = labels.float()\n",
    "            else:\n",
    "                labels = label.flatten()\n",
    "            if -1 in labels:\n",
    "                labels = (labels + 1) / 2\n",
    "            # if loss_fn.__class__.__name__ == \"CrossEntropyLoss\":\n",
    "            #     labels = labels.long()\n",
    "\n",
    "            # non_zero_ids = None\n",
    "            # if model.__class__.__name__ == \"GNAM\":\n",
    "            #     if val_mask:\n",
    "            #         labels = labels[data.val_mask]\n",
    "            #         non_zero_ids = torch.nonzero(data.val_mask).flatten()\n",
    "            #     else:\n",
    "            #         labels = labels[data.test_mask]\n",
    "            #         non_zero_ids = torch.nonzero(data.test_mask).flatten()\n",
    "\n",
    "            # forward\n",
    "            # if non_zero_ids is not None:\n",
    "            #     outputs = model.forward(inputs, non_zero_ids)\n",
    "            # else:\n",
    "            outputs = model.forward(graph, graph.ndata[\"feat\"])\n",
    "            # if not is_graph_task:\n",
    "            #     if val_mask:\n",
    "            #         outputs = outputs[data.val_mask]\n",
    "            #         labels = labels[data.val_mask]\n",
    "            # else:\n",
    "\n",
    "            n_samples += len(labels)\n",
    "            if outputs.dim() == 2 and outputs.shape[-1] == 1:\n",
    "                loss = loss_fn(outputs.flatten(), labels.float())\n",
    "            else:\n",
    "                loss = loss_fn(outputs, labels.float())\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if classify:\n",
    "                running_acc += get_accuracy(outputs, labels)\n",
    "            if compute_auc:\n",
    "                probas = torch.sigmoid(outputs).view(-1)\n",
    "                all_probas = np.concatenate((all_probas, probas.detach().cpu().numpy()))\n",
    "                all_labels = np.concatenate((all_labels, labels.detach().cpu().numpy()))\n",
    "\n",
    "        if compute_auc:\n",
    "            auc = roc_auc_score(all_labels, all_probas)\n",
    "        if classify:\n",
    "            if compute_auc:\n",
    "                return running_loss / len(dloader), running_acc / n_samples, auc\n",
    "            else:\n",
    "                return running_loss / len(dloader), running_acc / n_samples, -1\n",
    "        else:\n",
    "            return running_loss / len(dloader), -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    loss, tran_acc, _ = train_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        classify=True,\n",
    "        compute_auc=False,\n",
    "        epoch=epoch + 1,\n",
    "    )\n",
    "\n",
    "    valid_loss, accuracy, auc = test_epoch(model, valid_loader, loss_fn, compute_auc=True)\n",
    "    print(\n",
    "        f\"Epoch {epoch+1} | loss : {loss:.4f} |loss, tran_acc : {tran_acc:.4f} |valid_loss : {loss:.4f} | valid_accuracy : {accuracy:4f} | valid_auc : {auc:4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DGLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
