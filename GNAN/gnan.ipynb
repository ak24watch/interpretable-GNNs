{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import dgl.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import  roc_auc_score\n",
    "from dgl.nn.pytorch.glob import SumPooling\n",
    "from dataLoader import getData\n",
    "import numpy as np\n",
    "# Set print options to print the full tensor\n",
    "torch.set_printoptions(threshold=float(\"inf\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        num_layers,\n",
    "        hidden_channels=None,\n",
    "        bias=True,\n",
    "        dropout=0.0,\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.out_channels = out_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.dropout = dropout\n",
    "        self.fs = nn.ModuleList()\n",
    "\n",
    "        for _ in range(in_channels):\n",
    "            if num_layers == 1:\n",
    "                curr_f = [nn.Linear(1, out_channels, bias=bias)]\n",
    "            else:\n",
    "                curr_f = [\n",
    "                    nn.Linear(1, hidden_channels, bias=bias),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(p=dropout),\n",
    "                ]\n",
    "                for _ in range(1, num_layers - 1):\n",
    "                    curr_f.append(\n",
    "                        nn.Linear(hidden_channels, hidden_channels, bias=bias)\n",
    "                    )\n",
    "                    curr_f.append(nn.ReLU())\n",
    "                    curr_f.append(nn.Dropout(p=dropout))\n",
    "                curr_f.append(nn.Linear(hidden_channels, out_channels, bias=bias))\n",
    "            self.fs.append(nn.Sequential(*curr_f))\n",
    "\n",
    "    def init_params(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fx = torch.empty(x.size(0), x.size(1), self.out_channels).to(self.device)\n",
    "        for feature_index in range(x.size(1)):\n",
    "            feature_col = x[:, feature_index]\n",
    "            feature_col = feature_col.view(-1, 1)\n",
    "            feature_col = self.fs[feature_index](feature_col)\n",
    "            fx[:, feature_index] = feature_col\n",
    "\n",
    "        f_sums = fx.sum(dim=1)\n",
    "        return f_sums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorGNAN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        num_layers,\n",
    "        hidden_channels=None,\n",
    "        bias=True,\n",
    "        dropout=0.0,\n",
    "        device=\"cpu\",\n",
    "        limited_m=True,\n",
    "        normalize_m=True,\n",
    "        is_graph_task=False,\n",
    "        readout_n_layers=1,\n",
    "        final_agg=\"sum\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.out_channels = out_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.dropout = dropout\n",
    "        self.limited_m = limited_m\n",
    "        self.normalize_m = normalize_m\n",
    "        self.fs = nn.ModuleList()\n",
    "        self.is_graph_task = is_graph_task\n",
    "        self.readout_n_layers = readout_n_layers\n",
    "\n",
    "        self.actual_output_dim_f = (\n",
    "            1 if is_graph_task and readout_n_layers > 0 else out_channels\n",
    "        )\n",
    "        self.actual_output_dim_m = (\n",
    "            1 if limited_m or (is_graph_task and readout_n_layers > 0) else out_channels\n",
    "        )\n",
    "\n",
    "        for _ in range(in_channels):\n",
    "            if num_layers == 1:\n",
    "                curr_f = [nn.Linear(1, self.actual_output_dim_f, bias=bias)]\n",
    "            else:\n",
    "                curr_f = [\n",
    "                    nn.Linear(1, hidden_channels, bias=bias),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(p=dropout),\n",
    "                ]\n",
    "                for _ in range(1, num_layers - 1):\n",
    "                    curr_f.append(\n",
    "                        nn.Linear(hidden_channels, hidden_channels, bias=bias)\n",
    "                    )\n",
    "                    curr_f.append(nn.ReLU())\n",
    "                    curr_f.append(nn.Dropout(p=dropout))\n",
    "                curr_f.append(\n",
    "                    nn.Linear(hidden_channels, self.actual_output_dim_f, bias=bias)\n",
    "                )\n",
    "            self.fs.append(nn.Sequential(*curr_f))\n",
    "\n",
    "        m_bias = True\n",
    "        if is_graph_task:\n",
    "            m_bias = False\n",
    "        if num_layers == 1:\n",
    "            self.m = [nn.Linear(1, self.actual_output_dim_m, bias=m_bias)]\n",
    "\n",
    "        else:\n",
    "            self.m = [nn.Linear(1, hidden_channels, bias=m_bias), nn.ReLU()]\n",
    "            for _ in range(1, num_layers - 1):\n",
    "                self.m.append(nn.Linear(hidden_channels, hidden_channels, bias=m_bias))\n",
    "                self.m.append(nn.ReLU())\n",
    "            self.m.append(\n",
    "                nn.Linear(hidden_channels, self.actual_output_dim_m, bias=m_bias)\n",
    "            )\n",
    "        self.m = nn.Sequential(*self.m)\n",
    "\n",
    "        if is_graph_task and self.readout_n_layers > 0:\n",
    "            self.readout_nam = NAM(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                readout_n_layers,\n",
    "                hidden_channels,\n",
    "                bias,\n",
    "                dropout,\n",
    "                device,\n",
    "            )\n",
    "\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "\n",
    "    def forward(self, inputs, feats):\n",
    "        x, node_distances = (\n",
    "            feats,\n",
    "            inputs.ndata[\"distance_matrix\"],\n",
    "        )\n",
    "        fx = torch.empty(x.size(0), x.size(1), self.actual_output_dim_f).to(self.device) # (N, D, out)\n",
    "        for feature_index in range(x.size(1)):\n",
    "            feature_col = x[:, feature_index]\n",
    "            feature_col = feature_col.view(-1, 1)\n",
    "            feature_col = self.fs[feature_index](feature_col)\n",
    "            fx[:, feature_index] = feature_col\n",
    "\n",
    "        fx_perm = torch.permute(fx, (2, 0, 1))\n",
    "        if self.normalize_m:\n",
    "            node_distances = torch.div(node_distances, inputs.ndata[\"normalization_distance_matrix\"])\n",
    "        m_dist = self.m(node_distances.flatten().view(-1, 1)).view(\n",
    "            x.size(0), x.size(0), self.actual_output_dim_m\n",
    "        )\n",
    "        m_dist_perm = torch.permute(m_dist, (2, 0, 1))\n",
    "\n",
    "        mf = torch.matmul(m_dist_perm, fx_perm)\n",
    "\n",
    "        if not self.is_graph_task:\n",
    "            out = torch.sum(mf, dim=2)\n",
    "\n",
    "        else:\n",
    "            hidden = torch.sum(mf, dim=1)\n",
    "            if self.readout_n_layers > 0:\n",
    "                out = self.readout_nam(hidden)\n",
    "            else:\n",
    "                out = torch.sum(hidden, dim=1).view(1, -1)\n",
    "        return out.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNAN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim,\n",
    "        out_dim,\n",
    "        bias=True,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.distance_transform = nn.ModuleList()\n",
    "        self.feature_transform = nn.ModuleList()\n",
    "\n",
    "        for _ in range(in_dim):\n",
    "            self.distance_transform.append(nn.Linear(1, out_dim))\n",
    "            self.feature_transform.append(nn.Linear(1, out_dim))\n",
    "\n",
    "    def forward(self, graph, feats):\n",
    "        \"\"\"\n",
    "        params:\n",
    "\n",
    "            dis_matrix: shape (N,N) where N is number of nodes each element coresponds to node pair distance\n",
    "            feats: shape (N, d) where d is the feature dim\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        distance_matrix = graph.ndata[\"distance_matrix\"]  # (N, N)\n",
    "        normalization_distance_matrix = graph.ndata[\n",
    "            \"normalization_distance_matrix\"\n",
    "        ]  # N, N\n",
    "        num_nodes, feat_dim = feats.shape\n",
    "        f_matrix = torch.empty(feat_dim, num_nodes, self.out_dim)  # F matrix\n",
    "        m_matrix = torch.empty(feat_dim, num_nodes, num_nodes, self.out_dim)  # M matrix\n",
    "        # distance_matrix = torch.div(distance_matrix, normalization_distance_matrix)\n",
    "        distance_matrix = distance_matrix.view(-1, 1)  # (N*N,1)\n",
    "        for k in range(feat_dim):\n",
    "            # x_k is the kth feature of all nodes\n",
    "            x_k = feats[:, k].view(-1, 1)  # shape (N, 1)\n",
    "            f_matrix[k, :, :] = F.relu(self.feature_transform[k](x_k))  # (N, out)\n",
    "            m_matrix[k, :, :, :] = F.relu(self.distance_transform[k](distance_matrix).view(\n",
    "                num_nodes, num_nodes, -1\n",
    "            ) ) # (N, N, out)\n",
    "            # print(f\"m_matrix[k, :, :, :]shape {m_matrix[k, :, :, :].shape}\")\n",
    "            # m_matrix[k, :, :, :] = torch.div(\n",
    "            #     m_matrix[k, :, :, :], normalization_distance_matrix.unsqueeze(-1)\n",
    "            # )\n",
    "        # torch.set_printoptions(threshold=float(\"inf\"))\n",
    "        # print(f\"m matrix is {m_matrix}\")\n",
    "        m_matrix = torch.div(\n",
    "            m_matrix, normalization_distance_matrix.unsqueeze(-1).unsqueeze(0)\n",
    "        )\n",
    "        # print(f\"m_matrix after divsion is{m_matrix}\")\n",
    "        # f matrix shape (d, N, out)\n",
    "        # m matrix shape (d, N, N, out)\n",
    "        f_matrix = f_matrix.permute(2, 0, 1).unsqueeze(-1)  # (out,d, N, 1)\n",
    "        m_matrix = m_matrix.permute(3, 0, 1, 2)  # (out,d, N, N)\n",
    "        m_f_matrix = torch.matmul(m_matrix, f_matrix)  # (out, d, N, 1)\n",
    "        h = m_f_matrix.sum(1)  # (out, N, 1)\n",
    "        h = h.permute(2, 1, 0)  # (1,N,out)\n",
    "        h = h.squeeze(0)  # (N,out)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNANMODULE(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hiddem_dim=24, num_layers=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hiddem_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.pool = SumPooling()\n",
    "        self.gnan = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                self.gnan.append(GNAN(in_dim, hiddem_dim))\n",
    "            elif i == num_layers - 1:\n",
    "                self.gnan.append(GNAN(hiddem_dim, out_dim))\n",
    "            else:\n",
    "                self.gnan.append(GNAN(hiddem_dim, hiddem_dim))\n",
    "\n",
    "    def forward(self, graph, feats):\n",
    "        for i in range(self.num_layers):\n",
    "            if i == 0:\n",
    "                h = F.relu(self.gnan[i](graph, feats))\n",
    "                h = F.dropout(h,0.2)\n",
    "            elif i == self.num_layers - 1:\n",
    "                h = self.gnan[i](graph, h)\n",
    "            else:\n",
    "                h = F.relu(self.gnan[i](graph, h))\n",
    "                h = F.dropout(h,0.2)\n",
    "\n",
    "        h = self.pool(graph, h)\n",
    "        # print(\"class\")\n",
    "        # print(h)\n",
    "        # print(h.shape)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a373k/Documents/Github/interpretable-GNNs/GNAN/dataLoader.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.unique(torch.tensor(labels)).tolist()\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader, test_loader, num_feats, num_class = getData()\n",
    "# num_class = data.num_classes\n",
    "# Define model, loss function, and optimizer\n",
    "model = GNANMODULE(num_feats, 1, hiddem_dim=28)\n",
    "# model = TensorGNAN(num_feats,1,1,28, is_graph_task=True)\n",
    "loss_fn = nn.BCEWithLogitsLoss()  # Binary Cross Entropy with Logits\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "# Define training settings\n",
    "num_epochs = 100\n",
    "# train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_accuracy(outputs, labels):\n",
    "    if outputs.dim() == 2 and outputs.shape[-1] > 1:\n",
    "        return get_multiclass_accuracy(outputs, labels)\n",
    "    else:\n",
    "        y_prob = torch.sigmoid(outputs).view(-1)\n",
    "        y_prob = y_prob > 0.5\n",
    "        return (labels == y_prob).sum().item()\n",
    "\n",
    "\n",
    "def get_multiclass_accuracy(outputs, labels):\n",
    "    assert outputs.size(1) >= labels.max().item() + 1\n",
    "    probas = torch.softmax(outputs, dim=-1)\n",
    "    preds = torch.argmax(probas, dim=-1)\n",
    "    correct = (preds == labels).sum()\n",
    "    acc = correct\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model,\n",
    "    dloader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    classify=True,\n",
    "    label_index=0,\n",
    "    compute_auc=False,\n",
    "    is_graph_task=True,\n",
    "    epoch=-1\n",
    "):\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        running_loss = 0.0\n",
    "        n_samples = 0\n",
    "        all_probas = np.array([])\n",
    "        all_labels = np.array([])\n",
    "        if classify:\n",
    "            running_acc = 0.0\n",
    "   \n",
    "        for graph, label in dloader:\n",
    "    \n",
    "            if len(label.shape) > 1:\n",
    "                labels = label[:, label_index].view(-1, 1).flatten()\n",
    "                labels = labels.float()\n",
    "            else:\n",
    "                labels = label.flatten()\n",
    "            if -1 in labels:\n",
    "                labels = (labels + 1) / 2\n",
    "            # if loss_fn.__class__.__name__ == \"CrossEntropyLoss\":\n",
    "            #     labels = labels.long()\n",
    "\n",
    "            # non_zero_ids = None\n",
    "            # if model.__class__.__name__ == \"GNAM\":\n",
    "            #     labels = labels[data.train_mask]\n",
    "            #     non_zero_ids = torch.nonzero(data.train_mask).flatten()\n",
    "            # data = data.to(device)\n",
    "            # labels = labels.to(device)\n",
    "            # optimizer.zero_grad()\n",
    "            # if non_zero_ids is not None:\n",
    "            #     outputs = model.forward(data, non_zero_ids)\n",
    "            # else:\n",
    "            outputs = model.forward(graph, graph.ndata[\"feat\"])\n",
    "            # print(\"train model output\")\n",
    "            # print(outputs)\n",
    "            \n",
    "            # Check for NaN in the outputs\n",
    "            # if torch.isnan(outputs).any():\n",
    "            #     print(f\"NaN detected in model output at epoch{epoch}\")\n",
    "            #     break\n",
    "            \n",
    "            # if not is_graph_task:\n",
    "            #         labels = labels[data.train_mask]\n",
    "            #         outputs = outputs[data.train_mask]\n",
    "\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "\n",
    "            n_samples += len(labels)\n",
    "            if outputs.dim() == 2 and outputs.shape[-1] == 1:\n",
    "                loss = loss_fn(outputs.flatten(), labels.float())\n",
    "            else:\n",
    "                loss = loss_fn(outputs, labels)\n",
    "            # if torch.isnan(loss).any():\n",
    "            #     print(f\"NaN detected in loss at epoch {epoch}\")\n",
    "            #     break\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if compute_auc:\n",
    "                probas = torch.sigmoid(outputs).view(-1)\n",
    "                all_probas = np.concatenate((all_probas, probas.detach().cpu().numpy()))\n",
    "                all_labels = np.concatenate((all_labels, labels.detach().cpu().numpy()))\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if classify:\n",
    "                running_acc += get_accuracy(outputs, labels)\n",
    "\n",
    "        if compute_auc:\n",
    "            auc = roc_auc_score(all_labels, all_probas)\n",
    "\n",
    "        if classify:\n",
    "            if compute_auc:\n",
    "                return running_loss / len(dloader), running_acc / n_samples, auc\n",
    "            else:\n",
    "                return running_loss / len(dloader), running_acc / n_samples, -1\n",
    "        else:\n",
    "            return running_loss / len(dloader), -1\n",
    "\n",
    "\n",
    "def test_epoch(\n",
    "    model,\n",
    "    dloader,\n",
    "    loss_fn,\n",
    "    classify=True,\n",
    "    label_index=0,\n",
    "    compute_auc=False,\n",
    "    val_mask=False,\n",
    "    is_graph_task=True,\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.0\n",
    "        all_probas = np.array([])\n",
    "        all_labels = np.array([])\n",
    "        n_samples = 0\n",
    "        if classify:\n",
    "            running_acc = 0.0\n",
    "        model.eval()\n",
    "        for graph, label in dloader:\n",
    "            if len(label.shape) > 1:\n",
    "                labels = label[:, label_index].view(-1, 1).flatten()\n",
    "                labels = labels.float()\n",
    "            else:\n",
    "                labels = label.flatten()\n",
    "            if -1 in labels:\n",
    "                labels = (labels + 1) / 2\n",
    "            # if loss_fn.__class__.__name__ == \"CrossEntropyLoss\":\n",
    "            #     labels = labels.long()\n",
    "\n",
    "            # non_zero_ids = None\n",
    "            # if model.__class__.__name__ == \"GNAM\":\n",
    "            #     if val_mask:\n",
    "            #         labels = labels[data.val_mask]\n",
    "            #         non_zero_ids = torch.nonzero(data.val_mask).flatten()\n",
    "            #     else:\n",
    "            #         labels = labels[data.test_mask]\n",
    "            #         non_zero_ids = torch.nonzero(data.test_mask).flatten()\n",
    "\n",
    "            # forward\n",
    "            # if non_zero_ids is not None:\n",
    "            #     outputs = model.forward(inputs, non_zero_ids)\n",
    "            # else:\n",
    "            outputs = model.forward(graph, graph.ndata[\"feat\"])\n",
    "            # if not is_graph_task:\n",
    "            #     if val_mask:\n",
    "            #         outputs = outputs[data.val_mask]\n",
    "            #         labels = labels[data.val_mask]\n",
    "            # else:\n",
    "\n",
    "            n_samples += len(labels)\n",
    "            if outputs.dim() == 2 and outputs.shape[-1] == 1:\n",
    "                loss = loss_fn(outputs.flatten(), labels.float())\n",
    "            else:\n",
    "                loss = loss_fn(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if classify:\n",
    "                running_acc += get_accuracy(outputs, labels)\n",
    "            if compute_auc:\n",
    "                probas = torch.sigmoid(outputs).view(-1)\n",
    "                all_probas = np.concatenate((all_probas, probas.detach().cpu().numpy()))\n",
    "                all_labels = np.concatenate((all_labels, labels.detach().cpu().numpy()))\n",
    "\n",
    "        if compute_auc:\n",
    "            auc = roc_auc_score(all_labels, all_probas)\n",
    "        if classify:\n",
    "            if compute_auc:\n",
    "                return running_loss / len(dloader), running_acc / n_samples, auc\n",
    "            else:\n",
    "                return running_loss / len(dloader), running_acc / n_samples, -1\n",
    "        else:\n",
    "            return running_loss / len(dloader), -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a373k/Desktop/DGL_VENV/DGLenv/lib/python3.12/site-packages/torch/autograd/graph.py:768: UserWarning: Error detected in BinaryCrossEntropyWithLogitsBackward0. Traceback of forward call that caused the error:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/a373k/Desktop/DGL_VENV/DGLenv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/a373k/Desktop/DGL_VENV/DGLenv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/a373k/Desktop/DGL_VENV/DGLenv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/a373k/Desktop/DGL_VENV/DGLenv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/a373k/Desktop/DGL_VENV/DGLenv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/a373k/Desktop/DGL_VENV/DGLenv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/a373k/Desktop/DGL_VENV/DGLenv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/a373k/Desktop/DGL_VENV/DGLenv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/a373k/Desktop/DGL_VENV/DGLenv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/a373k/Desktop/DGL_VENV/DGLenv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/a373k/Desktop/DGL_VENV/DGLenv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/a373k/Desktop/DGL_VENV/DGLenv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/a373k/Desktop/DGL_VENV/DGLenv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/a373k/Desktop/DGL_VENV/DGLenv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/a373k/Desktop/DGL_VENV/DGLenv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/a373k/Desktop/DGL_VENV/DGLenv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/a373k/Desktop/DGL_VENV/DGLenv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_128333/797609742.py\", line 2, in <module>\n",
      "    loss, tran_acc, _ = train_epoch(\n",
      "  File \"/tmp/ipykernel_128333/24369575.py\", line 78, in train_epoch\n",
      "    loss = loss_fn(outputs.flatten(), labels.float())\n",
      "  File \"/home/a373k/Desktop/DGL_VENV/DGLenv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/a373k/Desktop/DGL_VENV/DGLenv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/a373k/Desktop/DGL_VENV/DGLenv/lib/python3.12/site-packages/torch/nn/modules/loss.py\", line 734, in forward\n",
      "    return F.binary_cross_entropy_with_logits(input, target,\n",
      "  File \"/home/a373k/Desktop/DGL_VENV/DGLenv/lib/python3.12/site-packages/torch/nn/functional.py\", line 3244, in binary_cross_entropy_with_logits\n",
      "    return torch.binary_cross_entropy_with_logits(input, target, weight, pos_weight, reduction_enum)\n",
      " (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:111.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'BinaryCrossEntropyWithLogitsBackward0' returned nan values in its 0th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 2\u001b[0m     loss, tran_acc, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclassify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompute_auc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     valid_loss, accuracy, auc \u001b[38;5;241m=\u001b[39m test_epoch(model, valid_loader, loss_fn)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m |loss, tran_acc : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtran_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m |valid_loss : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | valid_accuracy : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | valid_auc : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[7], line 85\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dloader, loss_fn, optimizer, classify, label_index, compute_auc, is_graph_task, epoch)\u001b[0m\n\u001b[1;32m     80\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# if torch.isnan(loss).any():\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m#     print(f\"NaN detected in loss at epoch {epoch}\")\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m#     break\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_auc:\n",
      "File \u001b[0;32m~/Desktop/DGL_VENV/DGLenv/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/DGL_VENV/DGLenv/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/DGL_VENV/DGLenv/lib/python3.12/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function 'BinaryCrossEntropyWithLogitsBackward0' returned nan values in its 0th output."
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    loss, tran_acc, _ = train_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        classify=True,\n",
    "        compute_auc=False,\n",
    "        epoch=epoch + 1,\n",
    "    )\n",
    "\n",
    "    valid_loss, accuracy, auc = test_epoch(model, valid_loader, loss_fn)\n",
    "    print(\n",
    "        f\"loss : {loss:.4f} |loss, tran_acc : {tran_acc:.4f} |valid_loss : {loss:.4f} | valid_accuracy : {accuracy:4f} | valid_auc : {auc:4f}\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DGLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
